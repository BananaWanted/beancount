#!/usr/bin/env python3
"""
Read a beancount input file and a directory name, and attempt to identify and
convert the files in the directory. There is also an option to automatically
file succesfully detected and converted input files.
"""
import textwrap
import io
import sys
import itertools
import os
import re
import datetime
from os import path
import mimetypes
import logging
import subprocess
from pprint import pprint
from xml.etree import ElementTree
import bs4
import collections
import csv
from pprint import pprint

from beancount2.parser import load
from beancount2.core import data
from beancount2.core.data import Account, Posting, Transaction, Check, Note, Decimal, Lot, Amount
from beancount2.core.data import account_from_name, format_entry
from beancount2.core.inventory import Position
from beancount2 import utils



# FIXME: This needs to be an option for the importer.
FLAG_IMPORT = data.FLAG_OKAY













#
# Getting the file types.
#

EXTRA_FILE_TYPES = [
    (re.compile(regexp, re.I), filetype)
    for regexp, filetype in (
            (r'.*\.qbo$', 'application/vnd.intu.qbo'),
            (r'.*\.(qfx|ofx)$', 'application/x-ofx'),
    )]

try:
    import magic
except ImportError:
    magic = None

def guess_file_type(filename):
    """Attempt to guess the type of the input file.
    Return a suitable mimetype, or None if we don't know."""

    # Try the regular mimetypes association.
    filetype, _ = mimetypes.guess_type(filename, False)

    if filetype is None:
        # Try out some extra ones that we know about.
        for regexp, mimtype in EXTRA_FILE_TYPES:
            if regexp.match(filename):
                filetype = mimtype
                break

    # FIXME: Add python-magic, optionally (if imported).
    if filetype is None:
        if not magic:
            pass # FIXME: issue a warning
        else:
            # Okay, we couldn't figure it out from the filename; use libmagic
            # (if installed).
            bfiletype = magic.from_file(filename, mime=True)
            filetype = bfiletype.decode()

    return filetype






#
# Identification of files to specific accounts.
#


def sliced_match(string):
    """Return a regexp that will match the given string with possibly any number of
    spaces in between. For example, '123' would become '1 *2 * 3'."""
    return '[ -]*'.join(string)


def identify_string(text, account_ids):
    """Given some string 'text', find if any of the account-ids in the 'account_ids'
    map is present in the text and return the corresponding account, or None."""

    for account_id, account in account_ids.items():
        mo = re.search(sliced_match(account_id), text)
        if mo:
            return account_id, account


def identify_pdf(filename, account_ids):
    "Attempt to identify the account for the given PDF file."""

    p = subprocess.Popen(('pdftotext', filename, '-'),
                         shell=False,
                         stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = p.communicate()
    if p.returncode != 0 or stderr:
        logging.error("Error running pdftotext: {}".format(stderr))
        return

    text = stdout.decode()
    return identify_string(text, account_ids)


def identify_csv(filename, account_ids):
    "Attempt to identify the account for the given CSV file."""
    text = open(filename).read()
    return identify_string(text, account_ids)


def ofx_get_account(node):
    "Given a beautifulsoup node, get the corresponding account id."
    acctid = node.find('acctid')
    # There's some garbage in here sometimes; clean it up.
    return acctid.text.split('\n')[0]


def identify_ofx(filename, account_ids):
    "Attempt to identify the account for the given OFX file."""

    soup = bs4.BeautifulSoup(open(filename), 'lxml')
    acctid = ofx_get_account(soup)
    try:
        return (acctid, account_ids[acctid])
    except KeyError:
        return None


IDENTIFY_HANDLERS = {
    'application/pdf'          : identify_pdf,
    'text/csv'                 : identify_csv,
    'application/x-ofx'        : identify_ofx,
    'application/vnd.intu.qbo' : identify_ofx,
}


def identify_account(filename, entries):
    """Given a filename, return the filetype and account that this file corresponds
    to, or None if it could not be identified."""

    filetype = guess_file_type(filename)

    identify_fun = IDENTIFY_HANDLERS.get(filetype, None)
    if identify_fun is None:
        # No handler was found; bail out.
        return

    account_ids = data.get_account_ids(entries)
    account_id, account = identify_fun(filename, account_ids)

    return filetype, account_id













#
# OFX Parser.
#

def souptodict(node):
    """Convert all of the child nodes from BeautifulSoup node into a dict.
    This assumes the direct children are uniquely named, but this is often the
    case."""
    return {child.name: child.contents[0].strip()
            for child in node.contents
            if isinstance(child, bs4.element.Tag)}

def soup_get(node, name, conversion=None):
    "Find a child anywhere below node and return its value or None."
    child = node.find(name)
    if child:
        value = child.contents[0]
        if conversion:
            value = conversion(value)
        return value


def parse_ofx_time(ofx_date_str):
    "Parse an OFX time string and return a datetime object.."
    if len(ofx_date_str) < 14:
        return datetime.datetime.strptime(ofx_date_str[:8], '%Y%m%d')
    else:
        return datetime.datetime.strptime(ofx_date_str[:14], '%Y%m%d%H%M%S')


def get_securities(soup):
    """Extract the list of securities from the OFX file."""

    seclistmsgsrsv = soup.find('seclistmsgsrsv1')
    if not seclistmsgsrsv:
        return

    securities = []
    for secinfo in seclistmsgsrsv.find_all('secinfo'):
        # Merge the two nodes in a dictionary.
        secid = souptodict(secinfo.find('secid'))
        secname = souptodict(secinfo.find('secname'))
        secid.update(secname)
        securities.append(secid)

    return securities


def import_ofx(filename, config, entries):
    """Extract transaction info from the given OFX file into transactions for the
    given account. This function returns a list of entries possibly partially
    filled entries, and a dictionary of annotations to be attached to entries
    and postings.
    """
    new_entries = []
    annotations = {}

    # Parse the XML file.
    soup = bs4.BeautifulSoup(open(filename), 'lxml')

    # Get the description of securities used in this file.
    securities = get_securities(soup)
    if securities:
        securities_map = {security['uniqueid']: security
                          for security in securities}

    # For each statement.
    txn_counter = itertools.count()
    for stmtrs in soup.find_all(re.compile('.*stmtrs$')):
        # account_type = st.find('accttype').text.strip()
        # bank_id = st.find('bankid').text.strip()

        # For each currnecy.
        for currency_node in stmtrs.find_all('curdef'):
            currency = currency_node.contents[0].strip()

            # Extract account-wide information.
            acctid = ofx_get_account(stmtrs)

            # Attempt to get an account from the ledger entries.
            account_name = config['asset']

            # Process all regular or credit-card transaction lists.
            for tranlist in stmtrs.find_all(re.compile('(|cc)tranlist')):
                ## print(tranlist.prettify())

                # Process the transactions from that list.
                for stmttrn in tranlist.find_all('stmttrn'):
                    trndict = souptodict(stmttrn)

                    # Build the transaction.
                    # date_start = parse_ofx_time(trndict['dtstart']).date()
                    # date_end = parse_ofx_time(trndict['dtend']).date()

# FIXME: Debug this with TD, the XML is crap
                    print(stmttrn.prettify())
                    print(trndict)

                    date = parse_ofx_time(trndict['dtposted']).date()
                    fileloc = data.FileLocation(filename, next(txn_counter))
                    payee = None
                    narration = ' / '.join(filter(None, (trndict.get(x, None) for x in ('trntype', 'name', 'memo'))))
                    entry = Transaction(fileloc, date, FLAG_IMPORT, payee, narration, None, None, [])

                    # Create a posting for it.
                    position = Position(Lot(currency, None, None), Decimal(trndict['trnamt']))
                    account = data.account_from_name(account_name)
                    entry.postings.append(Posting(entry, account, position, None, None))

                    new_entries.append(entry)


#FIXME: Farm this out to a Vanguard-specific function.
#FIXME: This needs to move into an import config and be provided as input.
            # FIXME: This needs to be parameterized.
            source_subaccounts = {
                'PRETAX' : config['asset_pretax'],
                'MATCH'  : config['asset_match'],
            }
            def get_cash_account(trantype, source, incometype):
                if trantype == 'BUYMF' and source == 'MATCH':
                    return config['income_match']
                elif trantype == 'REINVEST':
                    return config['dividend']
                elif trantype == 'TRANSFER':
                    return config['fees']
                else:
                    return config['asset_cash']


# FIXME: Make this into a function its own

            # Process all investment transaction lists.
            # Note: this was developed for Vanguard.
            for invtranlist in stmtrs.find_all(re.compile('invtranlist')):

                for tran in invtranlist.find_all(re.compile('(buymf|sellmf|reinvest|buystock|sellstock|buyopt|sellopt|transfer)')):

                    date = parse_ofx_time(soup_get(tran, 'dttrade')).date()
                    # date = parse_ofx_time(trndict['dtsettle']).date()

                    uniqueid = soup_get(tran, 'uniqueid')
                    security = securities_map[uniqueid]['ticker']

                    units = soup_get(tran, 'units', Decimal)
                    unitprice = soup_get(tran, 'unitprice', Decimal)
                    total = soup_get(tran, 'total', Decimal)

                    fileloc = data.FileLocation(filename, next(txn_counter))
                    payee = None

                    trantype = tran.name.upper()
                    incometype = soup_get(tran, 'incometype')
                    source = soup_get(tran, 'inv401ksource')
                    memo = soup_get(tran, 'memo')
                    narration = ' - '.join(filter(None, (trantype, incometype, source, memo)))

                    entry = Transaction(fileloc, date, FLAG_IMPORT, payee, narration, None, None, [])

                    # Create a posting for it.
                    tferaction = soup_get(tran, 'tferaction')
                    if tferaction == 'OUT':
                        units = -units

                    position = Position(Lot(security, Amount(unitprice, currency), None), units)

                    account = data.account_from_name('{}:{}'.format(
                        source_subaccounts[source], security))
                    entry.postings.append(Posting(entry, account, position, None, None))

                    if total is None:
                        total = units * unitprice
                        ##print('{:.9f} {:.9f} = {:.9f}'.format(units, unitprice, total))

                    position = Position(Lot(currency, None, None), -total)
                    account = data.account_from_name(get_cash_account(trantype, source, incometype))
                    entry.postings.append(Posting(entry, account, position, None, None))

                    new_entries.append(entry)

# FIXME: Make this into a function its own

                # Process all positions, convert them to Check directives.
                # Note: this was developed for Vanguard.
                for invposlist in stmtrs.find_all('invposlist'):
                    for invpos in invposlist.find_all('invpos'):
                        date = parse_ofx_time(soup_get(invpos, 'dtpriceasof')).date()

                        uniqueid = soup_get(invpos, 'uniqueid')
                        security = securities_map[uniqueid]['ticker']

                        units = soup_get(invpos, 'units', Decimal)
                        unitprice = soup_get(invpos, 'unitprice', Decimal)

                        fileloc = data.FileLocation(filename, next(txn_counter))
                        source = soup_get(invpos, 'inv401ksource')
                        account = data.account_from_name('{}:{}'.format(
                            source_subaccounts[source], security))

                        amount = Amount(units, security)
                        new_entries.append(Check(fileloc, date, account, amount, None))

    new_entries.sort(key=lambda entry: entry.date)
    return new_centries, annotations











#
# OANDA Importer.
#

IGNORE_TRANSACTIONS = """
Buy Order
Sell Order
Change Margin
Change Order
Change Trade
Order Cancelled
Order Expired
Order Filled
""".strip().splitlines()

RELEVANT_TRANSACTIONS = """
API Fee
API License Fee
Wire Fee
Buy Market Filled
Close Position
Close Trade
Fund Deposit
Fund Deposit (Transfer)
Fund Withdrawal
Fund Withdrawal (Transfer)
Interest
Sell Market Filled
Stop Loss
Stop Loss (Cancelled)
Take Profit
Trade Cancel
Buy Market
Sell Market
FXGlobalTransfer Sent
FXGlobalTransfer Fee
""".strip().splitlines()


def find_changing_types(filename):
    bytype = collections.defaultdict(list)
    for obj in utils.csv_dict_reader(filename):
        txntype = obj['transaction']
        bytype[txntype].append(obj)

    unchanging_types = set(bytype.keys())
    prev_balance = Decimal()
    for obj in utils.csv_dict_reader(filename):
        balance = obj['balance'].strip()
        if balance and balance != prev_balance:
            if obj['transaction'] in unchanging_types:
                print(obj)
            unchanging_types.discard(obj['transaction'])
            prev_balance = balance

    print("Unchanging types:")
    for txntype in unchanging_types:
        print(txntype)
    print()

    print("Changing types:")
    changing_types = set(bytype.keys()) - unchanging_types
    for txntype in changing_types:
        print(txntype)
    print()


def get_number(obj, aname):
    str_value = obj[aname].strip()
    if str_value:
        return Decimal(str_value)
    else:
        return Decimal()


def is_balanced_txn(obj):
    "Return true if the transaction is supposed to be balanced."
    txntype = obj['transaction']
    return not (txntype.startswith('Fund ') or
                txntype.startswith('FXGlobalTransfer ') or
                txntype.startswith('API ') or
                re.search(txntype, 'Wire Fee') or
                re.search(txntype, 'FXGlobalTransfer Fee'))


ZERO = Decimal()
ONE_CENT = Decimal('0.01')


def guess_currency(filename):
    """Try to guess the base currency of the account.
    We use the first transaction with a deposit or something
    that does not involve an instrument."""
    for obj in utils.csv_dict_reader(filename):
        if re.match('[A-Z]+$', obj['pair']):
            return obj['pair']


def oanda_add_posting(entry, account, number, currency):
    position = Position(Lot(currency, None, None), number)
    posting = Posting(entry, account, position, None, None)
    entry.postings.append(posting)


def import_csv_oanda(filename, config, entries):
    new_entries = []
    annotations = {}

    max_diff = Decimal()
    ## return find_changing_types(filename)

    currency = guess_currency(filename)

    # Iterate over all the transactions in the OANDA account.
    prev_balance = Decimal('0')
    prev_date = datetime.date(1970, 1, 1)
    for lineno, obj in enumerate(utils.csv_dict_reader(filename)):
        txntype = obj['transaction']
        date = datetime.datetime.strptime(obj['date'], '%B %d %H:%M:%S %Y %Z').date()

        # Insert some Check entries every month or so.
        if date.month != prev_date.month:
            prev_date = date
            fileloc = data.FileLocation(filename, lineno)
            amount = Amount(prev_balance, currency)
            new_entries.append(Check(fileloc, date, config['asset'], amount, None))

        # Ignore certain ones that have no effect on the balance, they just
        # change our positions.
        if txntype in IGNORE_TRANSACTIONS:
            continue
        assert txntype in RELEVANT_TRANSACTIONS, txntype

        # Get the change amounts.
        interest = get_number(obj, 'interest')
        pnl      = get_number(obj, 'p_l')
        amount   = get_number(obj, 'amount')
        other    = None

        # If the amount of change is supposed to balance, assert the
        # interest/pnl components match the amount.
        if is_balanced_txn(obj):
            assert interest + pnl == amount, (interest, pnl, amount, obj)
        else:
            # Otherwise use the amount itself.
            assert interest == ZERO
            assert pnl == ZERO
            other = amount

            if re.search('Wire Fee', txntype):
                # The wire fee amount is inverted, and book to fees account.
                other = -other
                other_account = config['fees']

            elif re.search('Transfer', txntype):
                # For transfers, use the other OANDA account.
                other_account = config['transfer']
            else:
                # Otherwise, well... we just don't know.
                other_account = config['limbo']

        del amount

        # Compute the change in any case.
        change = pnl + interest + (other or ZERO)

        # Compute the running balance and cross-check against what they report.
        balance = get_number(obj, 'balance')
        computed_balance = prev_balance + change

        # # (Note: there appeared to be a bug in how the balance was reported in
        # # 2007 for "Trade Cancel" entries; for that one we don't check)
        # if (abs(computed_balance - balance) > ONE_CENT and
        #     not (txntype == 'Trade Cancel' and date.year == 2007)):
        #     print("CANCEL_BUG")
        #     print((prev_balance, balance, change), (computed_balance - balance))
        #     pprint(obj)
        #     # raise SystemExit

        # Save the balance to compute the next one.
        prev_balance = balance

        # Create the transaction.
        fileloc = data.FileLocation(filename, lineno)
        narration = '{} - {}  (@{})'.format(txntype, obj['pair'], obj['ticket'])

## FIXME: Complete rendering the links
        link = obj['tran_link'].strip()
        links = set([link]) if link else None
        if links:
            print(links)

        entry = Transaction(fileloc, date, FLAG_IMPORT, None, narration, None, None, [])

        # FIXME: Add the rates for transfers
        oanda_add_posting(entry, config['asset'], change, currency)
        if pnl != ZERO:
            oanda_add_posting(entry, config['pnl'], -pnl, currency)
        if interest != ZERO:
            oanda_add_posting(entry, config['interest'], -interest, currency)
        if other is not None:
            oanda_add_posting(entry, other_account, -other, currency)

        if len(entry.postings) < 2:
            continue

        new_entries.append(entry)



        diff = balance - computed_balance
        # if diff > 0.5:
        #     print('DIFF', diff)
        if diff > max_diff:
            max_diff = diff

        if 0:
            balance_entry = Note(entry.fileloc, entry.date, config['asset'],
                                 'balance {}  -  {}  =  {}'.format(balance,
                                                                   computed_balance,
                                                                   diff))
            new_entries.append(balance_entry)




        assert len(entry.postings) > 1, format_entry(entry)

        # FIXME: You need to compress the interest amounts together...
        # FIXME: Add some regularly spaced check directives (!) - EASY.

    # print(';;; MAX_DIFF', max_diff)
    # print()




    new_entries.sort(key=lambda entry: entry.date)


    if True:
        # Compress all the interest entries for a shorter and cleaner set of
        # imported transactions.
        new_entries = compress(new_entries, lambda entry: re.search('Interest', entry.narration))

    return new_entries, annotations

# FIXME: Temporary; these would have been done manually. For debugging now, ignore, while developing.
OANDA_IGNORED = set(['873690057', '980400948', '1051458865'])

# FIXME TODO:
# - Compress the interest (in a separate routine that is functional, operates on a list of entries...)
# - Check that ignored transactions have zero amounts
# - Render tran link with @, and the ticket number too, as @links from my system
# - Render the transaction prices
# - Automatically fill in accounts for API and deposits / transfers (based on the currency)
# - Add the positions taken into subaccounts, a large positive number and a large negative number, this should be possible, under e.g. 'Income:US:OANDA:Primary:Positions:EUR_USD'





















#
# Importers.
#

def import_csv(filename, config, entries):
    ## FIXME: You need to somehow identify the institution separately.
    return import_csv_oanda(filename, config, entries)

IMPORTER_HANDLERS = {
    'text/csv'                 : import_csv,
    'application/x-ofx'        : import_ofx,
    'application/vnd.intu.qbo' : import_ofx,
}






























# Handler:
# - rename with import date
# - import transactions

#   - Write some code to detect when a transaction is already there
#   - Write some code to convert from entries into text

def main():
    import argparse, logging
    logging.basicConfig(level=logging.INFO, format='%(levelname)-8s: %(message)s')

    parser = argparse.ArgumentParser(__doc__.strip())
    parser.add_argument('-m', '--mindate', metavar='DATA', action='store',
                        help="Filter entries before this date.")
    parser.add_argument('filename', help='Beancount input file')
    parser.add_argument('config', help='Importer config file')
    parser.add_argument('directories', nargs='+', help='Directories to inspect')
    opts = parser.parse_args()

    if opts.mindate:
        mo = re.match('(\d\d\d\d)-(\d\d)-(\d\d)', opts.mindate)
        if not mo:
            parser.error("Invalid date format; must be YYYY-MM-DD")
        opts.mindate = datetime.date(*map(int, mo.groups()))

    # Load up the importer config.
    config = {}
    config_module = exec(compile(open(opts.config).read(), opts.config, 'exec'), globals(), config)
    IMPORTER_CONFIG = config['IMPORTER_CONFIG']

    # Parse the ledger and get the entries from it.
    entries, errors, options = load(opts.filename, quiet=True)

    for filename in utils.walk_files_or_dirs(opts.directories):

        # Figure out the account's filetype and account-id.
        identification = identify_account(filename, entries)
        if not identification:
            continue # Skip file.
        filetype, account_id = identification

        # We need to guess the institution too, somehow.


        # FIXME: Maybe the identification triple should be (mimetype, institution, account-id)?

        # Get the relevant importer function/module.
        importer = IMPORTER_HANDLERS.get(filetype)
        if importer is None:
            continue # No importer available for this filetype.

        institution = 'oanda'  ## FIXME: We need a routine to guess the institution


        # Get the relevant importer config.
        key = (institution, filetype, account_id)
        accounts = IMPORTER_CONFIG[key]

        # Run the importer.
        new_entries, annotations = importer(filename, accounts, entries)

        # Filter out entries with dates before 'mindate'.
        if opts.mindate:
            new_entries = list(itertools.dropwhile(lambda x: x.date < opts.mindate,
                                                   new_entries))

        # Find potential matching entries.
        duplicate_entries = find_duplicate_entries(new_entries, entries)

        print(';;')
        print(';; {}'.format(filename))
        print(';; ({}, {}, {})'.format(institution, filetype, account_id))
        print(';;\n')


        for entry in new_entries:
            entry_string = format_entry(entry)

            # Indicate that this entry may be a duplicate.
            if entry in duplicate_entries:
                print(';;;; POTENTIAL DUPLICATE ENTRY')
                entry_string = textwrap.indent(entry_string, ';; ')

            print(entry_string)


# FIXME: Write a routine that will allow each importer to declare the list of
# accounts that it requires in order to run.














# FIXME: Move in core/diff.py somewhere, this is purely an operation on a list of entries that returns a filtered list.

def find_duplicate_entries(new_entries, entries):
    """Find which entries from 'new_entries' are potential duplicates of
    'entries'."""

    duplicates = []

    window_size = utils.ONEDAY * 2
    EPSILON_PERC = 0.05

    # Create a map of entries by date, for easy lookup.
    date_index = utils.groupby(lambda entry: entry.date, entries)

    # For each of the new entries, look at entries at a nearby date.
    for new_entry in utils.filter_type(new_entries, Transaction):

        # Compute a mapping of accounts -> amounts.
        new_amounts = collections.defaultdict(Decimal)
        for posting in new_entry.postings:
            new_amounts[posting.account.name] += posting.position.number

        # Iterate over the window dates.
        for date in utils.iter_dates(new_entry.date - window_size,
                                     new_entry.date + window_size):

            for entry in date_index[date]:

                # Only consider entries of the same type.
                if type(entry) is not type(new_entry):
                    continue

                # Compute a mapping of accounts -> amounts.
                amounts = collections.defaultdict(Decimal)
                for posting in entry.postings:
                    amounts[posting.account.name] += posting.position.number

                # Look for amounts on common accounts.
                common_amounts = {}
                for new_account, new_amount in new_amounts.items():
                    amount = amounts.get(new_account, None)
                    if amount:
                        dsub = float(new_amount - amount)
                        dsum = float(new_amount + amount)
                        diff = dsub / dsum if dsum != 0 else EPSILON_PERC
                        if abs(diff) >= EPSILON_PERC:
                            continue
                        else:
                            # We found at least one common account with a close
                            # amount. Close enough.
                            duplicates.append(new_entry)

                            ## FIXME: Remove
                            # if 0:
                            #     print(dsub, dsum, diff, file=sys.stderr)
                            #     print(format_entry(new_entry), file=sys.stderr)
                            #     print(format_entry(entry), file=sys.stderr)
                            #     print('\n\n\n\n', file=sys.stderr)

                            break

    return duplicates


def compress(entries, predicate):
    """Replace consecutive sequences of Transaction entries that fulfill the given
    predicate by a single entry at the date of the last entry found. 'predicate'
    is a function that accepts an entry as an argument and returns a boolean,
    true if the entry should be compressed.

    This can be used to simply a list of transactions that are similar and occur
    frequently. As an example, in a retail FOREX trading account, differential
    interest of very small amounts is paid every day; it is not relevant to look
    at the full detail of this interest unless there are other transactions. You
    can use this to compress it into single entries between other types of
    transactions.
    """

    new_entries = []

    pending = []
    for entry in entries:
        if isinstance(entry, Transaction) and predicate(entry):
            # Save for compressing later.
            pending.append(entry)
        else:
            # Compress and output all the pending entries.
            if pending:
                new_entries.append(merge(pending, pending[-1]))
                pending.clear()

            # Output the differing entry.
            new_entries.append(entry)

    if pending:
        new_entries.append(merge(pending, pending[-1]))

    return new_entries


def merge(entries, prototype_entry):
    """Merge the given entries into a single entry with the characteristics of the
    prototype. Return the new entry. The combined list of postings are merged if
    everything about the postings is the same except the number.
    """

    # Aggregate the postings together.
    postings_map = collections.defaultdict(Decimal)
    for entry in entries:
        for posting in entry.postings:
            # Strip the number off the posting to act as an aggregation key.
            lot = posting.position.lot
            key = Posting(None, posting.account, lot, posting.price, posting.flag)

            postings_map[key] += posting.position.number

    # Create a new transaction with the aggregated postings.
    new_entry = Transaction(prototype_entry.fileloc,
                            prototype_entry.date,
                            prototype_entry.flag,
                            prototype_entry.payee,
                            prototype_entry.narration,
                            None, None, [])

    for posting, number in sorted(postings_map.items()):
        lot = posting.position
        position = Position(lot, number)
        new_entry.postings.append(
            Posting(new_entry,
                    posting.account, position, posting.price, posting.flag))

    return new_entry




























if __name__ == '__main__':
    main()
